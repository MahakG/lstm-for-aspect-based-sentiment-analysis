{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu1\"    \n",
    "import theano\n",
    "# theano.config.device = 'gpu' # Compute using GPU\n",
    "# theano.config.floatX = 'float32'\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "print theano.config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indices_to_one_hot_encodings(index, vector_length):\n",
    "    return [[1, 0] if i == index else [0, 1] for i in xrange(vector_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3906\n"
     ]
    }
   ],
   "source": [
    "# Load and process treebank data\n",
    "\n",
    "treebank_file1 = open('json/OPTA-treebank-0.1.json')\n",
    "treebank_file2 = open('skladnica_output.json')\n",
    "treebank = chain(list(json.load(treebank_file1)), list(json.load(treebank_file2)))\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for entry in treebank:\n",
    "    tree = entry['parsedSent']\n",
    "    words = []\n",
    "    sentiment = None\n",
    "    for index, node in enumerate(tree):\n",
    "        word = node.split('\\t')[1].lower()\n",
    "        words.append(word)\n",
    "        if node.split('\\t')[10] == 'S':\n",
    "            sentiment = index\n",
    "    if sentiment:\n",
    "        X.append(words)\n",
    "        y.append(indices_to_one_hot_encodings(sentiment, len(words)))\n",
    "\n",
    "dataset_length = len(X)\n",
    "slicing_point = int(dataset_length*0.9)\n",
    "\n",
    "X_train_raw = X[:slicing_point]\n",
    "y_train_raw = y[:slicing_point]\n",
    "X_test_raw = X[slicing_point+1:]\n",
    "y_test_raw = y[slicing_point+1:]\n",
    "\n",
    "treebank_vocabulary = set(chain(*X))\n",
    "print len(treebank_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'raczej', u'nie', u'dla', u'm\\u0142odych', u'ch\\u0142opc\\xf3w', u'.']\n",
      "[[0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train_raw[2]\n",
    "print y_train_raw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('w2v_allwiki_nkjp300_200.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'zapachnie'), (1, u'PORADNI'), (2, u'Fitelberga'), (3, u'komedianta'), (4, u'Zaprzesta\\u0107'), (5, u'Nampo'), (6, u'Schloendorff'), (7, u'zn\\u0119kanym'), (8, u'synkopy'), (9, u'unifikacji')]\n"
     ]
    }
   ],
   "source": [
    "# Import w2v's dictionary to a bag-of-words model\n",
    "w2v_vocabulary = Dictionary()\n",
    "w2v_vocabulary.doc2bow(w2v_model.vocab.keys(), allow_update=True)\n",
    "print w2v_vocabulary.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize dicts for representing w2v's dictionary as indices and 200-dim vectors\n",
    "w2indx = {v: k+1 for k, v in w2v_vocabulary.items()}\n",
    "w2vec = {word: w2v_model[word] for word in w2indx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_vocabulary_size = len(w2indx) + 1\n",
    "w2v_vocabulary_dimension = len(w2vec.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51615, 277138, 416148, 422622, 318134, 584324, 176240, 503788, 0]\n"
     ]
    }
   ],
   "source": [
    "def map_treebank_words_to_w2v_indices(treebank_data, w2indx):\n",
    "    treebank_data_vec = []\n",
    "    for sentence in treebank_data:\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vectorized_sentence.append(w2indx[word])\n",
    "            except KeyError:  # words absent in w2v model will be indexed as 0s\n",
    "                vectorized_sentence.append(0)\n",
    "        treebank_data_vec.append(vectorized_sentence)\n",
    "    return treebank_data_vec \n",
    "\n",
    "X_train = map_treebank_words_to_w2v_indices(X_train_raw, w2indx)\n",
    "X_test = map_treebank_words_to_w2v_indices(X_test_raw, w2indx)\n",
    "\n",
    "print X_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define numpy weights matrix for embedding layer\n",
    "embedding_weights = np.zeros((w2v_vocabulary_size , w2v_vocabulary_dimension))\n",
    "for word, index in w2indx.items():\n",
    "    embedding_weights[index, :] = w2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max sentence length\n",
    "max(\n",
    "    len(max(X_train, key=lambda sentence: len(sentence))),\n",
    "    len(max(X_test, key=lambda sentence: len(sentence)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0 580109 431241 193758 639684 453311      0]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize sequences length to 40 (will be extended with 0s)\n",
    "sentence_length = 40\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=sentence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=sentence_length)\n",
    "\n",
    "y_train = sequence.pad_sequences(y_train_raw, maxlen=sentence_length, value=[0, 1])\n",
    "y_test = sequence.pad_sequences(y_test_raw, maxlen=sentence_length, value=[0, 1])\n",
    "\n",
    "print X_train[2]\n",
    "print y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sentence_length,), dtype='int32')\n",
    "\n",
    "x = Embedding(\n",
    "    input_dim=w2v_vocabulary_size, \n",
    "    output_dim=w2v_vocabulary_dimension,\n",
    "    input_length=sentence_length,\n",
    "    mask_zero=True,\n",
    "    weights=[embedding_weights]\n",
    ")(inputs)\n",
    "\n",
    "lstm_out = LSTM(200, return_sequences=True)(x)\n",
    "\n",
    "regularized_data = Dropout(0.3)(lstm_out)\n",
    "\n",
    "predictions = TimeDistributed(Dense(2, activation='sigmoid'))(regularized_data)\n",
    "\n",
    "model = Model(input=inputs, output=predictions)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)               (None, 40)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)            (None, 40, 200)     141387200   input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                      (None, 40, 200)     320800      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)                (None, 40, 200)     0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistributed)(None, 40, 2)       402         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 141708402\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "# plot(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# theano.config.device = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1288 samples, validate on 143 samples\n",
      "Epoch 1/50\n",
      "44s - loss: 0.3807 - acc: 0.9374 - val_loss: 0.3047 - val_acc: 0.9717\n",
      "Epoch 2/50\n",
      "41s - loss: 0.2202 - acc: 0.9788 - val_loss: 0.2642 - val_acc: 0.9722\n",
      "Epoch 3/50\n",
      "49s - loss: 0.1542 - acc: 0.9825 - val_loss: 0.2567 - val_acc: 0.9706\n",
      "Epoch 4/50\n",
      "41s - loss: 0.1115 - acc: 0.9859 - val_loss: 0.2557 - val_acc: 0.9710\n",
      "Epoch 5/50\n",
      "43s - loss: 0.0820 - acc: 0.9882 - val_loss: 0.2557 - val_acc: 0.9705\n",
      "Epoch 6/50\n",
      "41s - loss: 0.0574 - acc: 0.9903 - val_loss: 0.2634 - val_acc: 0.9682\n",
      "Epoch 7/50\n",
      "39s - loss: 0.0399 - acc: 0.9912 - val_loss: 0.2757 - val_acc: 0.9680\n",
      "Epoch 8/50\n",
      "38s - loss: 0.0296 - acc: 0.9919 - val_loss: 0.2833 - val_acc: 0.9692\n",
      "Epoch 9/50\n",
      "39s - loss: 0.0232 - acc: 0.9919 - val_loss: 0.2933 - val_acc: 0.9682\n",
      "Epoch 10/50\n",
      "39s - loss: 0.0183 - acc: 0.9923 - val_loss: 0.3062 - val_acc: 0.9696\n",
      "Epoch 11/50\n",
      "39s - loss: 0.0172 - acc: 0.9919 - val_loss: 0.3138 - val_acc: 0.9694\n",
      "Epoch 12/50\n",
      "39s - loss: 0.0155 - acc: 0.9921 - val_loss: 0.3147 - val_acc: 0.9694\n",
      "Epoch 13/50\n",
      "38s - loss: 0.0141 - acc: 0.9920 - val_loss: 0.3213 - val_acc: 0.9687\n",
      "Epoch 14/50\n",
      "38s - loss: 0.0129 - acc: 0.9922 - val_loss: 0.3312 - val_acc: 0.9685\n",
      "Epoch 15/50\n",
      "38s - loss: 0.0118 - acc: 0.9923 - val_loss: 0.3437 - val_acc: 0.9692\n",
      "Epoch 16/50\n",
      "36s - loss: 0.0122 - acc: 0.9921 - val_loss: 0.3411 - val_acc: 0.9687\n",
      "Epoch 17/50\n",
      "35s - loss: 0.0107 - acc: 0.9923 - val_loss: 0.3461 - val_acc: 0.9692\n",
      "Epoch 18/50\n",
      "35s - loss: 0.0098 - acc: 0.9923 - val_loss: 0.3534 - val_acc: 0.9691\n",
      "Epoch 19/50\n",
      "35s - loss: 0.0108 - acc: 0.9923 - val_loss: 0.3583 - val_acc: 0.9701\n",
      "Epoch 20/50\n",
      "35s - loss: 0.0101 - acc: 0.9923 - val_loss: 0.3573 - val_acc: 0.9691\n",
      "Epoch 21/50\n",
      "35s - loss: 0.0105 - acc: 0.9923 - val_loss: 0.3617 - val_acc: 0.9696\n",
      "Epoch 22/50\n",
      "35s - loss: 0.0103 - acc: 0.9922 - val_loss: 0.3587 - val_acc: 0.9692\n",
      "Epoch 23/50\n",
      "35s - loss: 0.0093 - acc: 0.9923 - val_loss: 0.3609 - val_acc: 0.9691\n",
      "Epoch 24/50\n",
      "35s - loss: 0.0085 - acc: 0.9925 - val_loss: 0.3684 - val_acc: 0.9696\n",
      "Epoch 25/50\n",
      "35s - loss: 0.0093 - acc: 0.9922 - val_loss: 0.3804 - val_acc: 0.9703\n",
      "Epoch 26/50\n",
      "35s - loss: 0.0088 - acc: 0.9925 - val_loss: 0.3830 - val_acc: 0.9710\n",
      "Epoch 27/50\n",
      "35s - loss: 0.0101 - acc: 0.9922 - val_loss: 0.3679 - val_acc: 0.9694\n",
      "Epoch 28/50\n",
      "35s - loss: 0.0093 - acc: 0.9922 - val_loss: 0.3702 - val_acc: 0.9703\n",
      "Epoch 29/50\n",
      "35s - loss: 0.0094 - acc: 0.9922 - val_loss: 0.3696 - val_acc: 0.9703\n",
      "Epoch 30/50\n",
      "35s - loss: 0.0089 - acc: 0.9923 - val_loss: 0.3706 - val_acc: 0.9696\n",
      "Epoch 31/50\n",
      "35s - loss: 0.0083 - acc: 0.9923 - val_loss: 0.3777 - val_acc: 0.9698\n",
      "Epoch 32/50\n",
      "35s - loss: 0.0083 - acc: 0.9924 - val_loss: 0.3734 - val_acc: 0.9699\n",
      "Epoch 33/50\n",
      "35s - loss: 0.0080 - acc: 0.9924 - val_loss: 0.3790 - val_acc: 0.9701\n",
      "Epoch 34/50\n",
      "35s - loss: 0.0085 - acc: 0.9924 - val_loss: 0.3793 - val_acc: 0.9694\n",
      "Epoch 35/50\n",
      "36s - loss: 0.0078 - acc: 0.9926 - val_loss: 0.3816 - val_acc: 0.9692\n",
      "Epoch 36/50\n",
      "36s - loss: 0.0086 - acc: 0.9923 - val_loss: 0.3826 - val_acc: 0.9696\n",
      "Epoch 37/50\n",
      "36s - loss: 0.0077 - acc: 0.9924 - val_loss: 0.3765 - val_acc: 0.9689\n",
      "Epoch 38/50\n",
      "36s - loss: 0.0084 - acc: 0.9922 - val_loss: 0.3900 - val_acc: 0.9698\n",
      "Epoch 39/50\n",
      "34s - loss: 0.0086 - acc: 0.9922 - val_loss: 0.3902 - val_acc: 0.9703\n",
      "Epoch 40/50\n",
      "35s - loss: 0.0087 - acc: 0.9922 - val_loss: 0.3897 - val_acc: 0.9698\n",
      "Epoch 41/50\n",
      "35s - loss: 0.0082 - acc: 0.9921 - val_loss: 0.3896 - val_acc: 0.9696\n",
      "Epoch 42/50\n",
      "35s - loss: 0.0085 - acc: 0.9922 - val_loss: 0.3885 - val_acc: 0.9703\n",
      "Epoch 43/50\n",
      "35s - loss: 0.0086 - acc: 0.9922 - val_loss: 0.3933 - val_acc: 0.9705\n",
      "Epoch 44/50\n",
      "34s - loss: 0.0079 - acc: 0.9926 - val_loss: 0.3864 - val_acc: 0.9689\n",
      "Epoch 45/50\n",
      "35s - loss: 0.0082 - acc: 0.9923 - val_loss: 0.3892 - val_acc: 0.9703\n",
      "Epoch 46/50\n",
      "34s - loss: 0.0089 - acc: 0.9924 - val_loss: 0.3854 - val_acc: 0.9699\n",
      "Epoch 47/50\n",
      "35s - loss: 0.0082 - acc: 0.9922 - val_loss: 0.3949 - val_acc: 0.9699\n",
      "Epoch 48/50\n",
      "35s - loss: 0.0077 - acc: 0.9923 - val_loss: 0.3917 - val_acc: 0.9705\n",
      "Epoch 49/50\n",
      "35s - loss: 0.0089 - acc: 0.9922 - val_loss: 0.3902 - val_acc: 0.9708\n",
      "Epoch 50/50\n",
      "35s - loss: 0.0081 - acc: 0.9924 - val_loss: 0.3884 - val_acc: 0.9696\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_epoch = 50\n",
    "\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, \n",
    "                 validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# epochs = 10\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print('Epoch', i, '/', epochs)\n",
    "#     model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0.314685314685\n"
     ]
    }
   ],
   "source": [
    "def change_encoding_word(word):\n",
    "    return 1 if list(np.rint(word)) == [1, 0] else 0\n",
    "\n",
    "def change_encoding(one_hot_encoded_sentence):\n",
    "    # Switch from ndarray([[0.88, 0.11], [0.34, 0.98]]) encoding to [1, 0] encoding \n",
    "    # and finally index number\n",
    "    normalized_sentence = []\n",
    "    for word in one_hot_encoded_sentence:\n",
    "        normalized_sentence.append(change_encoding_word(word))\n",
    "    return normalized_sentence\n",
    "\n",
    "\n",
    "total_accuracy = 0\n",
    "for n, sentence in enumerate(predictions):\n",
    "    index_of_sentiment = np.argmax(change_encoding(sentence))\n",
    "    print change_encoding_word(y_test[n][index_of_sentiment])\n",
    "    total_accuracy += change_encoding_word(y_test[n][index_of_sentiment])\n",
    "    \n",
    "\n",
    "print total_accuracy/float(len(y_test))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
