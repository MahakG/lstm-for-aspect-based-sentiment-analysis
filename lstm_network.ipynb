{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indices_to_one_hot_encodings(index, vector_length):\n",
    "    return [[0, 1] if i == index else [0, 1] for i in xrange(vector_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3906\n"
     ]
    }
   ],
   "source": [
    "# Load and process treebank data\n",
    "\n",
    "treebank_file1 = open('json/OPTA-treebank-0.1.json')\n",
    "treebank_file2 = open('skladnica_output.json')\n",
    "treebank = chain(list(json.load(treebank_file1)), list(json.load(treebank_file2)))\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for entry in treebank:\n",
    "    tree = entry['parsedSent']\n",
    "    words = []\n",
    "    sentiment = None\n",
    "    for index, node in enumerate(tree):\n",
    "        word = node.split('\\t')[1].lower()\n",
    "        words.append(word)\n",
    "        if node.split('\\t')[10] == 'S':\n",
    "            sentiment = index\n",
    "    if sentiment:\n",
    "        X.append(words)\n",
    "        y.append(indices_to_one_hot_encodings(sentiment, len(words)))\n",
    "\n",
    "dataset_length = len(X)\n",
    "slicing_point = int(dataset_length*0.9)\n",
    "\n",
    "X_train_raw = X[:slicing_point]\n",
    "y_train_raw = y[:slicing_point]\n",
    "X_test_raw = X[slicing_point+1:]\n",
    "y_test_raw = y[slicing_point+1:]\n",
    "\n",
    "treebank_vocabulary = set(chain(*X))\n",
    "print len(treebank_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'raczej', u'nie', u'dla', u'm\\u0142odych', u'ch\\u0142opc\\xf3w', u'.']\n",
      "[[0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train_raw[2]\n",
    "print y_train_raw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('w2v_allwiki_nkjp300_200.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'zapachnie'), (1, u'PORADNI'), (2, u'Fitelberga'), (3, u'komedianta'), (4, u'Zaprzesta\\u0107'), (5, u'Nampo'), (6, u'Schloendorff'), (7, u'zn\\u0119kanym'), (8, u'synkopy'), (9, u'unifikacji')]\n"
     ]
    }
   ],
   "source": [
    "# Import w2v's dictionary to a bag-of-words model\n",
    "w2v_vocabulary = Dictionary()\n",
    "w2v_vocabulary.doc2bow(w2v_model.vocab.keys(), allow_update=True)\n",
    "print w2v_vocabulary.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize dicts for representing w2v's dictionary as indices and 200-dim vectors\n",
    "w2indx = {v: k+1 for k, v in w2v_vocabulary.items()}\n",
    "w2vec = {word: w2v_model[word] for word in w2indx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_vocabulary_size = len(w2indx) + 1\n",
    "w2v_vocabulary_dimension = len(w2vec.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51615, 277138, 416148, 422622, 318134, 584324, 176240, 503788, 0]\n"
     ]
    }
   ],
   "source": [
    "def map_treebank_words_to_w2v_indices(treebank_data, w2indx):\n",
    "    treebank_data_vec = []\n",
    "    for sentence in treebank_data:\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vectorized_sentence.append(w2indx[word])\n",
    "            except KeyError:  # words absent in w2v model will be indexed as 0s\n",
    "                vectorized_sentence.append(0)\n",
    "        treebank_data_vec.append(vectorized_sentence)\n",
    "    return treebank_data_vec \n",
    "\n",
    "X_train = map_treebank_words_to_w2v_indices(X_train_raw, w2indx)\n",
    "X_test = map_treebank_words_to_w2v_indices(X_test_raw, w2indx)\n",
    "\n",
    "print X_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define numpy weights matrix for embedding layer\n",
    "embedding_weights = np.zeros((w2v_vocabulary_size , w2v_vocabulary_dimension))\n",
    "for word, index in w2indx.items():\n",
    "    embedding_weights[index, :] = w2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max sentence length\n",
    "max(\n",
    "    len(max(X_train, key=lambda sentence: len(sentence))),\n",
    "    len(max(X_test, key=lambda sentence: len(sentence)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0 580109 431241 193758 639684 453311      0]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize sequences length to 40 (will be extended with 0s)\n",
    "sentence_length = 40\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=sentence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=sentence_length)\n",
    "\n",
    "y_train = sequence.pad_sequences(y_train_raw, maxlen=sentence_length)\n",
    "y_test = sequence.pad_sequences(y_test_raw, maxlen=sentence_length)\n",
    "\n",
    "print X_train[2]\n",
    "print y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sentence_length,), dtype='int32')\n",
    "\n",
    "x = Embedding(\n",
    "    input_dim=w2v_vocabulary_size, \n",
    "    output_dim=w2v_vocabulary_dimension,\n",
    "    input_length=sentence_length,\n",
    "    mask_zero=True,\n",
    "    weights=[embedding_weights]\n",
    ")(inputs)\n",
    "\n",
    "lstm_out = LSTM(200, return_sequences=True)(x)\n",
    "\n",
    "predictions = TimeDistributed(Dense(1, activation='sigmoid'))(lstm_out)\n",
    "\n",
    "model = Model(input=inputs, output=predictions)\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(input_dim=w2v_vocabulary_size,  # input: a sequence of word indices\n",
    "#                     output_dim=w2v_vocabulary_dimension,  # output: a sequence of word vectors (200-dim)\n",
    "#                     mask_zero=True,\n",
    "#                     weights=[embedding_weights])  #initialize weights with pre-trained embeddings\n",
    "#          )\n",
    "\n",
    "# model.add(LSTM(input_dim=w2v_vocabulary_dimension,  # input: a sequence of word vectors (200-dim)\n",
    "#                output_dim=w2v_vocabulary_dimension,  # output: a sequence of 200-dim vectors\n",
    "#                return_sequences=True))\n",
    "\n",
    "# model.add(Dropout(0.3))\n",
    "# preserves data dimensions\n",
    "\n",
    "# model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "# # apply Dense to each vector in a sequence\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)               (None, 40)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)            (None, 40, 200)     141387200   input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                      (None, 40, 200)     320800      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistributed)(None, 40, 1)       201         lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 141708201\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1288 samples, validate on 143 samples\n",
      "Epoch 1/4\n",
      "58s - loss: 0.6935 - acc: 0.1395 - val_loss: 0.6956 - val_acc: 0.1796\n",
      "Epoch 2/4\n",
      "57s - loss: 0.6933 - acc: 0.1395 - val_loss: 0.6955 - val_acc: 0.1796\n",
      "Epoch 3/4\n",
      "57s - loss: 0.6933 - acc: 0.1395 - val_loss: 0.6954 - val_acc: 0.1796\n",
      "Epoch 4/4\n",
      "57s - loss: 0.6932 - acc: 0.1395 - val_loss: 0.6953 - val_acc: 0.1796\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epoch = 4\n",
    "\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, \n",
    "                 validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
