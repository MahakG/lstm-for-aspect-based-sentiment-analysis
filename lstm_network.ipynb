{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indices_to_one_hot_encodings(index, vector_length):\n",
    "    return [1 if i == index else 0 for i in xrange(vector_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3906\n"
     ]
    }
   ],
   "source": [
    "# Load and process treebank data\n",
    "\n",
    "treebank_file1 = open('json/OPTA-treebank-0.1.json')\n",
    "treebank_file2 = open('skladnica_output.json')\n",
    "treebank = chain(list(json.load(treebank_file1)), list(json.load(treebank_file2)))\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for entry in treebank:\n",
    "    tree = entry['parsedSent']\n",
    "    words = []\n",
    "    sentiment = None\n",
    "    for index, node in enumerate(tree):\n",
    "        word = node.split('\\t')[1].lower()\n",
    "        words.append(word)\n",
    "        if node.split('\\t')[10] == 'S':\n",
    "            sentiment = index\n",
    "    if sentiment:\n",
    "        X.append(words)\n",
    "        y.append(indices_to_one_hot_encodings(sentiment, len(words)))\n",
    "\n",
    "dataset_length = len(X)\n",
    "slicing_point = int(dataset_length*0.9)\n",
    "\n",
    "X_train_raw = X[:slicing_point]\n",
    "y_train_raw = y[:slicing_point]\n",
    "X_test_raw = X[slicing_point+1:]\n",
    "y_test_raw = y[slicing_point+1:]\n",
    "\n",
    "treebank_vocabulary = set(chain(*X))\n",
    "print len(treebank_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'raczej', u'nie', u'dla', u'm\\u0142odych', u'ch\\u0142opc\\xf3w', u'.']\n",
      "[0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print X_train_raw[2]\n",
    "print y_train_raw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('w2v_allwiki_nkjp300_200.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'zapachnie'), (1, u'PORADNI'), (2, u'Fitelberga'), (3, u'komedianta'), (4, u'Zaprzesta\\u0107'), (5, u'Nampo'), (6, u'Schloendorff'), (7, u'zn\\u0119kanym'), (8, u'synkopy'), (9, u'unifikacji')]\n"
     ]
    }
   ],
   "source": [
    "# Import w2v's dictionary to a bag-of-words model\n",
    "w2v_vocabulary = Dictionary()\n",
    "w2v_vocabulary.doc2bow(w2v_model.vocab.keys(), allow_update=True)\n",
    "print w2v_vocabulary.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize dicts for representing w2v's dictionary as indices and 200-dim vectors\n",
    "w2indx = {v: k+1 for k, v in w2v_vocabulary.items()}\n",
    "w2vec = {word: w2v_model[word] for word in w2indx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_vocabulary_size = len(w2indx) + 1\n",
    "w2v_vocabulary_dimension = len(w2vec.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51615, 277138, 416148, 422622, 318134, 584324, 176240, 503788, 0]\n"
     ]
    }
   ],
   "source": [
    "def map_treebank_words_to_w2v_indices(treebank_data, w2indx):\n",
    "    treebank_data_vec = []\n",
    "    for sentence in treebank_data:\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vectorized_sentence.append(w2indx[word])\n",
    "            except KeyError:  # words absent in w2v model will be indexed as 0s\n",
    "                vectorized_sentence.append(0)\n",
    "        treebank_data_vec.append(vectorized_sentence)\n",
    "    return treebank_data_vec \n",
    "\n",
    "X_train = map_treebank_words_to_w2v_indices(X_train_raw, w2indx)\n",
    "X_test = map_treebank_words_to_w2v_indices(X_test_raw, w2indx)\n",
    "\n",
    "print X_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define numpy weights matrix for embedding layer\n",
    "embedding_weights = np.zeros((w2v_vocabulary_size , w2v_vocabulary_dimension))\n",
    "for word, index in w2indx.items():\n",
    "    embedding_weights[index, :] = w2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max sentence length\n",
    "max(\n",
    "    len(max(X_train, key=lambda sentence: len(sentence))),\n",
    "    len(max(X_test, key=lambda sentence: len(sentence)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0 580109 431241 193758 639684 453311      0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Normalize sequences length to 40 (will be extended with 0s)\n",
    "sentence_length = 40\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=sentence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=sentence_length)\n",
    "\n",
    "y_train = sequence.pad_sequences(y_train_raw, maxlen=sentence_length, value=0)\n",
    "y_test = sequence.pad_sequences(y_test_raw, maxlen=sentence_length, value=0)\n",
    "\n",
    "print X_train[2]\n",
    "print y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sentence_length,), dtype='int32')\n",
    "\n",
    "x = Embedding(\n",
    "    input_dim=w2v_vocabulary_size, \n",
    "    output_dim=w2v_vocabulary_dimension,\n",
    "    input_length=sentence_length,\n",
    "    mask_zero=True,\n",
    "    weights=[embedding_weights]\n",
    ")(inputs)\n",
    "\n",
    "lstm_out = LSTM(200, return_sequences=True)(x)\n",
    "\n",
    "regularized_data = Dropout(0.3)(lstm_out)\n",
    "\n",
    "predictions = TimeDistributed(Dense(1, activation='sigmoid'))(regularized_data)\n",
    "\n",
    "model = Model(input=inputs, output=predictions)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1288 samples, validate on 143 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Bad input argument to theano function with name \"/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/backend/theano_backend.py:489\"  at index 1(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (50, 40).')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-107ed6cebada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, \n\u001b[1;32m----> 6\u001b[1;33m                  validation_data=(X_test, y_test), verbose=2)\n\u001b[0m",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[0;32m    785\u001b[0m                             \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    175\u001b[0m             raise TypeError(\"Wrong number of dimensions: expected %s,\"\n\u001b[0;32m    176\u001b[0m                             \" got %s with shape %s.\" % (self.ndim, data.ndim,\n\u001b[1;32m--> 177\u001b[1;33m                                                         data.shape))\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maligned\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Bad input argument to theano function with name \"/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/backend/theano_backend.py:489\"  at index 1(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (50, 40).')"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_epoch = 4\n",
    "\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, \n",
    "                 validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[:100], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predictions[3]\n",
    "for i in range(len(pred)):\n",
    "    print pred[i], y_test[3][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
