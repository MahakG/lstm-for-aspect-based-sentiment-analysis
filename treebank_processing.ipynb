{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def index_to_one_hot_encoding(index, vector_length):\n",
    "    return [1 if i == index else 0 for i in xrange(vector_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3906\n"
     ]
    }
   ],
   "source": [
    "treebank_file1 = open('json/OPTA-treebank-0.1.json')\n",
    "treebank_file2 = open('skladnica_output.json')\n",
    "treebank = chain(list(json.load(treebank_file1)), list(json.load(treebank_file2)))\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for entry in treebank:\n",
    "    tree = entry['parsedSent']\n",
    "    words = []\n",
    "    sentiment = None\n",
    "    for index, node in enumerate(tree):\n",
    "        word = node.split('\\t')[1].lower()\n",
    "        words.append(word)\n",
    "        if node.split('\\t')[10] == 'S':\n",
    "            sentiment = index\n",
    "    if sentiment:\n",
    "        X.append(words)\n",
    "        y.append(index_to_one_hot_encoding(sentiment, len(words)))\n",
    "\n",
    "# for a, b in zip(X, y):\n",
    "#     print ' '.join(a)\n",
    "#     print b\n",
    "\n",
    "dataset_length = len(X)\n",
    "slicing_point = int(dataset_length*0.9)\n",
    "\n",
    "X_train = X[:slicing_point]\n",
    "y_train = y[:slicing_point]\n",
    "X_test = X[slicing_point+1:]\n",
    "y_test = y[slicing_point+1:]\n",
    "\n",
    "treebank_vocabulary = set(chain(*X))\n",
    "print len(treebank_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "w2v_model = Word2Vec.load('w2v_allwiki_nkjp300_200.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706935"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vocabulary = Dictionary()\n",
    "w2v_vocabulary.doc2bow(w2v_model.vocab.keys(), allow_update=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2indx = {v: k+1 for k, v in w2v_vocabulary.items()}\n",
    "w2vec = {word: model[word] for word in w2indx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_vocabulary_size = len(w2indx) + 1\n",
    "w2v_vocabulary_dimension = len(w2vec.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "# our_dict = {}\n",
    "# missing_count = 0\n",
    "\n",
    "# for word in treebank_vocabulary:\n",
    "#     if word in w2v_model:\n",
    "#         our_dict[word] = w2v_model[word]\n",
    "#     else:\n",
    "#         missing_count += 1\n",
    "# #         print(\"missing key %s\" %(word)) \n",
    "#         our_dict[word] = np.zeros(w2v_vocabulary_dimension)\n",
    "# print missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_treebank_words_to_w2v_indices(treebank_data, w2indx):\n",
    "    treebank_data_vec = []\n",
    "    for sentence in treebank_data:\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vectorized_sentence.append(w2indx[word])\n",
    "            except KeyError:\n",
    "                vectorized_sentence.append(0)\n",
    "        treebank_data_vec.append(vectorized_sentence)\n",
    "    return treebank_data_vec \n",
    "\n",
    "X_train_vec = map_treebank_words_to_w2v_indices(X_train, w2indx)\n",
    "X_test_vec = map_treebank_words_to_w2v_indices(X_test, w2indx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((w2v_vocabulary_size , w2v_vocabulary_dimension))\n",
    "for word, index in w2indx.items():\n",
    "    embedding_weights[index, :] = w2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_length = 100\n",
    "X_train = sequence.pad_sequences(X_train_vec, maxlen=input_length)\n",
    "X_test = sequence.pad_sequences(X_test_vec, maxlen=input_length)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim=w2v_vocabulary_dimension,\n",
    "                    input_dim=w2v_vocabulary_size,\n",
    "                    mask_zero=True,\n",
    "                    weights=[embedding_weights],\n",
    "                    input_length=input_length))\n",
    "# input: a sequence of word indices\n",
    "# output: a sequence of word vectors (200-dim)\n",
    "\n",
    "model.add(LSTM(output_dim=w2v_vocabulary_dimension, return_sequences=True))\n",
    "# input: a sequence of word vectors (200-dim)\n",
    "# output: a sequence of 200-dim vectors\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "# preserves data dimensions\n",
    "\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "# apply Dense to each vector in a sequence, i.e. apply \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              class_mode='binary',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "A target array with shape (1288, 1) was passed for an output of shape (None, 100, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-231-89a870b9e0e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, \n\u001b[1;32m----> 6\u001b[1;33m                  validation_data=(X_test, y_test), verbose=2, show_accuracy=True)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m                                                            \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m                                                            batch_size=batch_size)\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[0;32m    909\u001b[0m                           in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n\u001b[0;32m    910\u001b[0m         \u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mcheck_loss_and_target_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_output_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomek/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, losses, output_shapes)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 raise Exception('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    200\u001b[0m                                 \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                                 \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                                 \u001b[1;34m'This loss expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                                 \u001b[1;34m'targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: A target array with shape (1288, 1) was passed for an output of shape (None, 100, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epoch = 5\n",
    "\n",
    "start_time = time()\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, \n",
    "                 validation_data=(X_test, y_test), verbose=2)\n",
    "end_time = time()\n",
    "\n",
    "print(\"Training took %f secs\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
